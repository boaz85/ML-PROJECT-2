{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Machine Learning - Project 2\n",
    "### Boaz Shvartzman, Ofir Ziv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-26T19:25:51.404529Z",
     "start_time": "2018-03-26T19:25:51.263675Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import collections\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-26T19:25:52.402929Z",
     "start_time": "2018-03-26T19:25:52.382297Z"
    }
   },
   "outputs": [],
   "source": [
    "class DatasetSplitParser(object):\n",
    "    \n",
    "    def __init__(self, split_path):\n",
    "        with open(split_path, 'r')as f:\n",
    "            file_content = f.read()\n",
    "\n",
    "        self._ds_by_index = {}\n",
    "        \n",
    "        for row in file_content.split('\\n')[1:-1]:\n",
    "            index, ds = row.split(',')\n",
    "            self._ds_by_index[int(index)] = int(ds)\n",
    "            \n",
    "    def get_dataset_by_index(self):\n",
    "        return self._ds_by_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetsHolder(object):\n",
    "    \n",
    "    def __init__(self, sentences_path, datasplit_parser):\n",
    "        with open(sentences_path, 'r')as f:\n",
    "            file_content = f.read()\n",
    "        \n",
    "        file_content = file_content.decode(\"ascii\", \"ignore\") # Remove non-ASCII\n",
    "        file_content = re.sub(r'([^\\s\\w]|_)+', '', file_content) # Remove non-alphanumeric\n",
    "        file_content = file_content.lower() # Lowercase\n",
    "        \n",
    "        self._trainset = {}\n",
    "        self._testset = {}\n",
    "        \n",
    "        ds_by_index = datasplit_parser.get_dataset_by_index()\n",
    "\n",
    "        for row in file_content.split('\\n')[1:-1]:\n",
    "            \n",
    "            index, sentence = row.split('\\t')\n",
    "            index, sentence = int(index), re.sub(r'\\b\\w{1,2}\\b', '', sentence).split()\n",
    "            \n",
    "            if len(sentence) < 2:\n",
    "                continue\n",
    "                \n",
    "            if ds_by_index[index] == 1:\n",
    "                self._trainset[index] = sentence\n",
    "                \n",
    "            elif ds_by_index[index] == 2:\n",
    "                self._testset[index] = sentence\n",
    "\n",
    "    def get_trainset(self):\n",
    "        return self._trainset\n",
    "    \n",
    "    def get_testset(self):\n",
    "        return self._testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "    1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hyperparameters(object):\n",
    "    \n",
    "    def __init__(self, window_size=1, vector_size=30, negative_words=50, iterations=2000,\n",
    "                 noise_distribution='unigram', noise_dist_params={'alpha': 3/4.}, random_seed=1234):\n",
    "\n",
    "        self.window_size = window_size\n",
    "        self.vector_size = vector_size\n",
    "        self.negative_words = negative_words\n",
    "        self.iterations = iterations\n",
    "        self.noise_distribution = noise_distribution\n",
    "        self.noise_dist_params = noise_dist_params\n",
    "        self.random_seed = random_seed\n",
    "        np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2. + 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelParameters(object):\n",
    "    \n",
    "    def __init__(self, hyperparameters):\n",
    "        self._hyperparameters = hyperparameters\n",
    "        \n",
    "    def init(self, trainset):\n",
    "        self._words = np.unique(sum(trainset.values(), []))\n",
    "        vector_size = self._hyperparameters.vector_size\n",
    "        \n",
    "        def sample():\n",
    "            vectors = np.random.multivariate_normal(np.zeros(vector_size), np.identity(vector_size) * 1e-2, len(self._words))\n",
    "            return vectors / np.sqrt(np.sum(np.power(vectors, 2), axis=1)).reshape(-1, 1)\n",
    "\n",
    "        self.U, self.V = sample(), sample()\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def word2index(self, words):\n",
    "        indices = np.where(np.isin(self._words, words))[0]\n",
    "        if isinstance(words, collections.Iterable) and not type(words) in [str, unicode]:\n",
    "            return indices\n",
    "        else:\n",
    "            return indices[0]\n",
    "\n",
    "    def index2word(self, indices):\n",
    "        return self._words[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    4. + 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class unigram_sampler(object):\n",
    "\n",
    "    def __init__(self, trainset, alpha):\n",
    "        self.words, self.frequencies = np.unique(sum(trainset.values(), []), return_counts=True)\n",
    "        factored = np.power(self.frequencies, alpha).astype(np.float64)\n",
    "        probabilities = factored / factored.sum()\n",
    "        self._cumsum = np.cumsum(probabilities)\n",
    "        \n",
    "    def __call__(self, k):\n",
    "        indices = [np.where(self._cumsum > np.random.rand())[0][0] for i in range(k)]\n",
    "        return self.words[indices]\n",
    "\n",
    "def get_words_sampler(dataset, hyperparams):\n",
    "    \n",
    "    if hyperparams.noise_distribution == 'unigram':\n",
    "        return unigram_sampler(dataset, hyperparams.noise_dist_params['alpha'])\n",
    "\n",
    "    else:\n",
    "        raise NotImplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ log \\left ( \\frac{1}{1 + exp(v_c^Tu_t)} \\right ) + \\sum_{j=1}^K log \\left (1 - \\frac{1}{1 + exp(v_j^Tu_t)} \\right ) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0 / (1 + np.exp(-z))\n",
    "\n",
    "def log_prob_context_with_negatives(U, V, target_word, context_word, negative_words, model_params):\n",
    "    sig = np.log(sigmoid(V[context_word].T.dot(U[target_word])))\n",
    "    neg = np.sum(np.log(1 - sigmoid(V[negative_words].T.dot(U[target_word]))))\n",
    "    \n",
    "    return sig + neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3\n",
    "\n",
    "    1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial f}{du_t} = \\left (1 - \\sigma(v_c^Tu_t \\right )v_c - \\sum_{j=1}^K \\left (1-\\sigma(v_j^Tu_t) \\right )v_j $$\n",
    "\n",
    "$$ \\frac{\\partial f}{dv_c} = \\left (1 - \\sigma(v_c^Tu_t \\right )u_t $$\n",
    "\n",
    "$$ \\frac{\\partial f}{dv_j} = - \\left (1 - \\sigma(v_j^Tu_t \\right )u_t $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prob_context_with_negatives_gradient(U, V, target_word, context_word, negative_words):\n",
    "    u_t, v_c = V[context_word].reshape(-1, 1), U[target_word].reshape(-1, 1)\n",
    "    neg = (1 - sigmoid(v_c.T.dot(u_t)))\n",
    "    \n",
    "    dFdu = neg * v_c - np.sum((1 - sigmoid(v_c.T.dot(u_t))) * V[negative_words], axis=0).reshape(-1, 1)\n",
    "    dFdv_c = neg * u_t\n",
    "    \n",
    "    neg = (1 - sigmoid(V[negative_words].dot(u_t))).reshape(-1, 1)\n",
    "    dFdv_j = (-neg).dot(u_t.T)\n",
    "    \n",
    "    return dFdu, dFdv_c, dFdv_j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_target_context(dataset, window_size):\n",
    "    w = np.random.choice(dataset.values(), 1)[0]\n",
    "    target = np.random.choice(range(len(w)))\n",
    "    \n",
    "    pairs = []\n",
    "    \n",
    "    for i in range(window_size):\n",
    "        if target + i < len(w):\n",
    "            pairs.append((w[target], w[target + i]))\n",
    "        if target - i >= 0:\n",
    "            pairs.append((w[target], w[target - i]))\n",
    "            \n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_context_minibatch(dataset, minibatch_size, window_size):\n",
    "    return [\n",
    "        sample_target_context(dataset, window_size) for i in range(minibatch_size)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameters_update(minibatch_samples, hyperparameters, model_params, U, V):\n",
    "\n",
    "    u_gradients = {}\n",
    "    c_gradients = {}\n",
    "    \n",
    "    for samples in minibatch_samples:\n",
    "        target_index = model_params.word2index(samples[0][0])\n",
    "        for target, context in samples:\n",
    "            context_index = model_params.word2index(context)\n",
    "            negative_indeices = model_params.word2index(sample_k_words(hyperparameters.negative_words))\n",
    "            \n",
    "            g_t, g_c, g_j = log_prob_context_with_negatives_gradient(U, V, target_index, context_index, \n",
    "                                                                     negative_indeices)\n",
    "\n",
    "            u_gradients[target_index] = u_gradients.get(target_index, 0) + g_t\n",
    "            c_gradients[context_index] = c_gradients.get(context_index, 0) + g_c\n",
    "            \n",
    "            for i, index in enumerate(negative_indeices):\n",
    "                c_gradients[index] = c_gradients.get(index, 0) + g_j[i].reshape(-1, 1)\n",
    "                \n",
    "    return u_gradients, c_gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDParameters(object):\n",
    "    \n",
    "    def __init__(self, learning_rate=1e-3, minibatch_size=50, anealing_factor=300):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.anealing_factor = anealing_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LearnParamsUsingSGD(trainset, hyperparameters, sgd_parameters, model_parameters):\n",
    "    \n",
    "    U, V = model_parameters.U, model_parameters.V\n",
    "    learning_rate = sgd_parameters.learning_rate\n",
    "    \n",
    "    for i in range(hyperparameters.iterations):\n",
    "        minibatch = get_target_context_minibatch(trainset, sgd_parameters.minibatch_size, hyperparameters.window_size)\n",
    "        u_gradients, c_gradients = get_parameters_update(minibatch, hyperparameters, model_parameters, U, V)\n",
    "        print i\n",
    "        if i % sgd_parameters.anealing_factor == 0:\n",
    "            learning_rate /= 2.0\n",
    "        \n",
    "        for index, gradient in u_gradients.items():\n",
    "            U[index] += (learning_rate * gradient).reshape(-1)\n",
    "\n",
    "        for index, gradient in c_gradients.items():\n",
    "            V[index] += (learning_rate * gradient).reshape(-1)\n",
    "            \n",
    "    return U, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_parser = DatasetSplitParser('datasetSplit.txt')\n",
    "dataset_holder = DatasetsHolder('datasetSentences.txt', split_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = Hyperparameters()\n",
    "model_parameters = ModelParameters(hyperparameters).init(dataset_holder.get_trainset())\n",
    "sample_k_words = get_words_sampler(dataset_holder.get_trainset(), hyperparameters)\n",
    "sgd_parameters = SGDParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-d35a2be31bbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLearnParamsUsingSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_holder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trainset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-36-41da002d95d4>\u001b[0m in \u001b[0;36mLearnParamsUsingSGD\u001b[0;34m(trainset, hyperparameters, sgd_parameters, model_parameters)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mminibatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_target_context_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminibatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mu_gradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_gradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_parameters_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msgd_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manealing_factor\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-8689edc0f18f>\u001b[0m in \u001b[0;36mget_parameters_update\u001b[0;34m(minibatch_samples, hyperparameters, model_params, U, V)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mcontext_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mnegative_indeices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_k_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnegative_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             g_t, g_c, g_j = log_prob_context_with_negatives_gradient(U, V, target_index, context_index, \n",
      "\u001b[0;32m<ipython-input-8-5a49de3f747f>\u001b[0m in \u001b[0;36mword2index\u001b[0;34m(self, words)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mword2index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/boazsh/.virtualenvs/ml_course/lib/python2.7/site-packages/numpy/lib/arraysetops.pyc\u001b[0m in \u001b[0;36misin\u001b[0;34m(element, test_elements, assume_unique, invert)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0melement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     return in1d(element, test_elements, assume_unique=assume_unique,\n\u001b[0;32m--> 578\u001b[0;31m                 invert=invert).reshape(element.shape)\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/boazsh/.virtualenvs/ml_course/lib/python2.7/site-packages/numpy/lib/arraysetops.pyc\u001b[0m in \u001b[0;36min1d\u001b[0;34m(ar1, ar2, assume_unique, invert)\u001b[0m\n\u001b[1;32m    466\u001b[0m     \u001b[0;31m# Otherwise use sorting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0massume_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m         \u001b[0mar1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrev_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m         \u001b[0mar2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/boazsh/.virtualenvs/ml_course/lib/python2.7/site-packages/numpy/lib/arraysetops.pyc\u001b[0m in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0mar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_unique1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Invalid axis kwarg specified for unique'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/boazsh/.virtualenvs/ml_course/lib/python2.7/site-packages/numpy/lib/arraysetops.pyc\u001b[0m in \u001b[0;36m_unique1d\u001b[0;34m(ar, return_index, return_inverse, return_counts)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moptional_indices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0mperm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mergesort'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_index\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'quicksort'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/Users/boazsh/.virtualenvs/ml_course/lib/python2.7/site-packages/numpy/lib/arraysetops.py\u001b[0m(274)\u001b[0;36m_unique1d\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    272 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    273 \u001b[0;31m    \u001b[0;32mif\u001b[0m \u001b[0moptional_indices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 274 \u001b[0;31m        \u001b[0mperm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mergesort'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_index\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'quicksort'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    275 \u001b[0;31m        \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    276 \u001b[0;31m    \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> q\n"
     ]
    }
   ],
   "source": [
    "U, V = LearnParamsUsingSGD(dataset_holder.get_trainset(), hyperparameters, sgd_parameters, model_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "305px",
    "left": "1213px",
    "right": "20px",
    "top": "154px",
    "width": "601px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
