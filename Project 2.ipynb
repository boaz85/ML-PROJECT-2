{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Machine Learning - Project 2\n",
    "### Boaz Shvartzman, Ofir Ziv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-26T19:25:51.404529Z",
     "start_time": "2018-03-26T19:25:51.263675Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import collections\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display, HTML\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-26T19:25:52.402929Z",
     "start_time": "2018-03-26T19:25:52.382297Z"
    }
   },
   "outputs": [],
   "source": [
    "class DatasetSplitParser(object):\n",
    "    \n",
    "    def __init__(self, split_path):\n",
    "        with open(split_path, 'r')as f:\n",
    "            file_content = f.read()\n",
    "\n",
    "        self._ds_by_index = {}\n",
    "        \n",
    "        for row in file_content.split('\\n')[1:-1]:\n",
    "            index, ds = row.split(',')\n",
    "            self._ds_by_index[int(index)] = int(ds)\n",
    "            \n",
    "    def get_dataset_by_index(self):\n",
    "        return self._ds_by_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetsHolder(object):\n",
    "    \n",
    "    def __init__(self, sentences_path, datasplit_parser):\n",
    "        with open(sentences_path, 'r')as f:\n",
    "            file_content = f.read()\n",
    "        \n",
    "        file_content = file_content.decode(\"ascii\", \"ignore\") # Remove non-ASCII\n",
    "        file_content = re.sub(r'([^\\s\\w]|_)+', '', file_content) # Remove non-alphanumeric\n",
    "        file_content = file_content.lower() # Lowercase\n",
    "        \n",
    "        self._trainset = {}\n",
    "        self._testset = {}\n",
    "        \n",
    "        ds_by_index = datasplit_parser.get_dataset_by_index()\n",
    "\n",
    "        for row in file_content.split('\\n')[1:-1]:\n",
    "            \n",
    "            index, sentence = row.split('\\t')\n",
    "            index, sentence = int(index), re.sub(r'\\b\\w{1,2}\\b', '', sentence).split()\n",
    "            \n",
    "            if len(sentence) < 2:\n",
    "                continue\n",
    "                \n",
    "            if ds_by_index[index] == 1:\n",
    "                self._trainset[index] = sentence\n",
    "                \n",
    "            elif ds_by_index[index] == 2:\n",
    "                self._testset[index] = sentence\n",
    "\n",
    "    def get_trainset(self):\n",
    "        return self._trainset\n",
    "    \n",
    "    def get_testset(self):\n",
    "        return self._testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "    1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hyperparameters(object):\n",
    "    \n",
    "    def __init__(self, window_size=1, vector_size=30, negative_words=50, iterations=2000,\n",
    "                 noise_distribution='unigram', noise_dist_params={'alpha': 1.}, random_seed=1234,\n",
    "                testset_measure_iterations=100):\n",
    "\n",
    "        self.window_size = window_size\n",
    "        self.vector_size = vector_size\n",
    "        self.negative_words = negative_words\n",
    "        self.iterations = iterations\n",
    "        self.noise_distribution = noise_distribution\n",
    "        self.noise_dist_params = noise_dist_params\n",
    "        self.random_seed = random_seed\n",
    "        self.testset_measure_iterations = testset_measure_iterations\n",
    "        np.random.seed(random_seed)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return '\\n'.join(['Window size: {}'.format(self.window_size),\n",
    "                          'Embedding vector size: {}'.format(self.vector_size),\n",
    "                          'Negative words: {}'.format(self.negative_words),\n",
    "                          'Iterations: {}'.format(self.iterations),\n",
    "                          'Noise distribution: {}'.format(self.noise_distribution),\n",
    "                          'Random seed: {}'.format(self.random_seed),\n",
    "                          'Testset measure step size: {}'.format(self.testset_measure_iterations)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2. + 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelParameters(object):\n",
    "    \n",
    "    def __init__(self, hyperparameters):\n",
    "        self._hyperparameters = hyperparameters\n",
    "        \n",
    "    def init(self, trainset):\n",
    "        self._words = np.unique(sum(trainset.values(), []))\n",
    "        vector_size = self._hyperparameters.vector_size\n",
    "        \n",
    "        def sample():\n",
    "            vectors = np.random.multivariate_normal(np.zeros(vector_size), np.identity(vector_size) * 1e-2, len(self._words))\n",
    "            return vectors / np.sqrt(np.sum(np.power(vectors, 2), axis=1)).reshape(-1, 1)\n",
    "\n",
    "        self.U, self.V = sample(), sample()\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def word2index(self, words):\n",
    "        indices = np.where(np.isin(self._words, words))[0]\n",
    "        if isinstance(words, collections.Iterable) and not type(words) in [str, unicode]:\n",
    "            return indices\n",
    "        else:\n",
    "            return indices[0]\n",
    "\n",
    "    def index2word(self, indices):\n",
    "        return self._words[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    4. + 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class unigram_sampler(object):\n",
    "\n",
    "    def __init__(self, trainset, alpha):\n",
    "        self.words, self.frequencies = np.unique(sum(trainset.values(), []), return_counts=True)\n",
    "        factored = np.power(self.frequencies, alpha).astype(np.float64)\n",
    "        self.probabilities = factored / factored.sum()\n",
    "        #self._cumsum = np.cumsum(probabilities)\n",
    "        \n",
    "    def __call__(self, k):\n",
    "        #indices = [np.where(self._cumsum > np.random.rand())[0][0] for i in range(k)]\n",
    "        return np.random.choice(self.words, k, p=self.probabilities)\n",
    "        #return self.words[indices]\n",
    "\n",
    "def get_words_sampler(dataset, hyperparams):\n",
    "    \n",
    "    if hyperparams.noise_distribution == 'unigram':\n",
    "        return unigram_sampler(dataset, hyperparams.noise_dist_params['alpha'])\n",
    "\n",
    "    else:\n",
    "        raise NotImplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ log \\left ( \\frac{1}{1 + exp(-v_c^Tu_t)} \\right ) + \\sum_{j=1}^K log \\left (1 - \\frac{1}{1 + exp(-v_j^Tu_t)} \\right ) = log \\left ( \\sigma(v_c^Tu_t) \\right ) + \\sum_{j=1}^K log \\left (1 - \\sigma(v_j^Tu_t) \\right )$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0 / (1 + np.exp(-z))\n",
    "\n",
    "def log_prob_context_with_negatives(target_word, context_word, negative_words, model_params):\n",
    "    U, V = model_params.U, model_params.V\n",
    "    u_t, v_c = U[target_word].reshape(-1, 1), V[context_word].reshape(-1, 1)\n",
    "    sig = np.log(sigmoid(v_c.T.dot(u_t)))\n",
    "    neg = np.sum(np.log(1 - sigmoid(V[negative_words].dot(u_t))))\n",
    "    \n",
    "    return (sig + neg).ravel()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3\n",
    "\n",
    "    1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial f}{du_t} = \\left (1 - \\sigma(v_c^Tu_t) \\right )v_c - \\sum_{j=1}^K \\sigma(v_j^Tu_t)v_j$$\n",
    "\n",
    "$$ \\frac{\\partial f}{dv_c} = \\left (1 - \\sigma(v_c^Tu_t) \\right )u_t $$\n",
    "\n",
    "$$ \\frac{\\partial f}{dv_j} = - \\sigma(v_j^Tu_t) u_t $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prob_context_with_negatives_gradient(model_params, target_word, context_word, negative_words):\n",
    "    U, V = model_params.U, model_params.V\n",
    "    u_t, v_c = U[target_word].reshape(-1, 1), V[context_word].reshape(-1, 1)\n",
    "    neg = 1 - sigmoid(v_c.T.dot(u_t))\n",
    "    \n",
    "    dFdu = neg * v_c - np.sum(sigmoid(V[negative_words].dot(u_t)) * V[negative_words], axis=0).reshape(-1, 1)\n",
    "    dFdv_c = neg * u_t\n",
    "\n",
    "    dFdv_j = -(sigmoid(V[[negative_words]].dot(u_t))).dot(u_t.T)\n",
    "    \n",
    "    return dFdu, dFdv_c, dFdv_j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetContextSampler(object):\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        self.values = dataset.values()\n",
    "        self.probs = np.array([len(v) for v in self.values]).astype(float)\n",
    "        self.probs /= self.probs.sum()\n",
    "        \n",
    "    def __call__(self, window_size):\n",
    "        \n",
    "        w = np.random.choice(self.values, 1, p=self.probs)[0]\n",
    "        target = np.random.choice(range(len(w)))\n",
    "\n",
    "        pairs = []\n",
    "\n",
    "        for i in range(1, window_size + 1):\n",
    "            if target + i < len(w):\n",
    "                pairs.append((w[target], w[target + i]))\n",
    "            if target - i >= 0:\n",
    "                pairs.append((w[target], w[target - i]))\n",
    "\n",
    "        return pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_context_minibatch(sampler, minibatch_size, window_size):\n",
    "    return [\n",
    "        sampler(window_size) for i in range(minibatch_size)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameters_update(minibatch_samples, hyperparameters, model_params):\n",
    "\n",
    "    u_gradients = {}\n",
    "    c_gradients = {}\n",
    "\n",
    "    for samples in minibatch_samples:\n",
    "        target_index = model_params.word2index(samples[0][0])\n",
    "\n",
    "        for target, context in samples:\n",
    "            context_index = model_params.word2index(context)\n",
    "            negative_indeices = model_params.word2index(sample_k_words(hyperparameters.negative_words))\n",
    "            \n",
    "            g_t, g_c, g_j = log_prob_context_with_negatives_gradient(model_params, target_index, context_index, \n",
    "                                                                     negative_indeices)\n",
    "            u_gradients[target_index] = u_gradients.get(target_index, 0) + g_t\n",
    "            c_gradients[context_index] = c_gradients.get(context_index, 0) + g_c\n",
    "            \n",
    "            for i, index in enumerate(negative_indeices):\n",
    "                c_gradients[index] = c_gradients.get(index, 0) + g_j[i].reshape(-1, 1)\n",
    "                \n",
    "    return u_gradients, c_gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDParameters(object):\n",
    "    \n",
    "    def __init__(self, learning_rate=1e-3, minibatch_size=50, anealing_factor=300):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.anealing_factor = anealing_factor\n",
    "        \n",
    "    def __str__(self):\n",
    "        return '\\n'.join(['Initial learning rate: {}'.format(self.learning_rate),\n",
    "                          'Minibatch size: {}'.format(self.minibatch_size),\n",
    "                          'Anealing step size: {}'.format(self.anealing_factor)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LearnParamsUsingSGD(trainset, hyperparameters, sgd_parameters, model_parameters):\n",
    "    \n",
    "    U, V = model_parameters.U, model_parameters.V\n",
    "    learning_rate = sgd_parameters.learning_rate\n",
    "    errors = []\n",
    "    sample_target_context = TargetContextSampler(trainset)\n",
    "    \n",
    "    for i in range(1, hyperparameters.iterations):\n",
    "        minibatch = get_target_context_minibatch(sample_target_context, sgd_parameters.minibatch_size, \n",
    "                                                 hyperparameters.window_size)\n",
    "        \n",
    "        u_gradients, c_gradients = get_parameters_update(minibatch, hyperparameters, model_parameters, U, V)\n",
    "\n",
    "        if i % sgd_parameters.anealing_factor == 0:\n",
    "            learning_rate /= 2.0\n",
    "        \n",
    "        for index, gradient in u_gradients.items():\n",
    "            U[index] += (learning_rate * gradient).reshape(-1)\n",
    "\n",
    "        for index, gradient in c_gradients.items():\n",
    "            V[index] += (learning_rate * gradient).reshape(-1)\n",
    "            \n",
    "        U /= np.sqrt(np.sum(np.power(U, 2), axis=1)).reshape(-1, 1)\n",
    "        V /= np.sqrt(np.sum(np.power(V, 2), axis=1)).reshape(-1, 1)\n",
    "            \n",
    "    return U, V, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4\n",
    "    1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prob_full(dataset, hyperparameters, model_params):\n",
    "    \n",
    "    def get_next_pair():\n",
    "        for _, w in dataset.iteritems():\n",
    "            for target in range(len(w)):\n",
    "                for i in range(1, hyperparameters.window_size + 1):\n",
    "                    if target + i < len(w):\n",
    "                        yield w[target], w[target + i]\n",
    "                    if target - i >= 0:\n",
    "                        yield w[target], w[target - i]\n",
    "    \n",
    "    total_log_prob = 0\n",
    "    num_of_pairs = 0\n",
    "    \n",
    "    for target_word, context_word in get_next_pair(): \n",
    "        try:\n",
    "            target_index, context_index = model_params.word2index(target_word), model_params.word2index(context_word)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        negative_words = model_params.word2index(sample_k_words(hyperparameters.negative_words))   \n",
    "        total_log_prob += log_prob_context_with_negatives(target_index, context_index, \n",
    "                                                          negative_words, model_params)\n",
    "        num_of_pairs += 1\n",
    "        \n",
    "    return total_log_prob, num_of_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prob_minibatch(minibatch_samples, hyperparameters, model_params):\n",
    "    \n",
    "    log_prob = 0\n",
    "    num_of_pairs = 0\n",
    "    \n",
    "    for samples in minibatch_samples:\n",
    "        target_index = model_params.word2index(samples[0][0])\n",
    "        \n",
    "        for target, context in samples:\n",
    "            context_index = model_params.word2index(context)\n",
    "            negative_indeices = model_params.word2index(sample_k_words(hyperparameters.negative_words))\n",
    "            log_prob += log_prob_context_with_negatives(target_index, context_index, \n",
    "                                                        negative_indeices, model_params)\n",
    "        num_of_pairs += 1\n",
    "        \n",
    "    return log_prob, num_of_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    3. + 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_live_view():\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.ion()\n",
    "    fig.show()\n",
    "    fig.canvas.draw()\n",
    "    return fig, ax\n",
    "\n",
    "def update_live_view(fig, ax, errors, anealing_factor, smooth_factor=30):\n",
    "    ax.clear()\n",
    "    ax.plot(range(len(errors)), errors, color='orange', alpha=0.5)\n",
    "    ret = np.cumsum(errors, dtype=float)\n",
    "    ret[smooth_factor:] = ret[smooth_factor:] - ret[:-smooth_factor]\n",
    "    rolling = ret[smooth_factor - 1:] / smooth_factor\n",
    "    ax.plot(range(smooth_factor, len(rolling) + smooth_factor), rolling, color='green')\n",
    "    for line in range(0, len(errors), anealing_factor)[1:]:\n",
    "        plt.axvline(x=line, color='red')\n",
    "    fig.canvas.draw()\n",
    "\n",
    "def LearnParamsUsingSGD(trainset, hyperparameters, sgd_parameters, model_parameters, testset=None, liveview=False):\n",
    "\n",
    "    U, V = model_parameters.U, model_parameters.V\n",
    "    learning_rate = sgd_parameters.learning_rate\n",
    "    scores_sequence = []\n",
    "    sample_target_context = TargetContextSampler(trainset)\n",
    "    trainset_scores, testset_scores = {}, {}\n",
    "    \n",
    "    if liveview:\n",
    "        fig, ax = init_live_view()\n",
    "    \n",
    "    for i in range(1, hyperparameters.iterations):\n",
    "\n",
    "        minibatch = get_target_context_minibatch(sample_target_context, sgd_parameters.minibatch_size, \n",
    "                                                 hyperparameters.window_size)\n",
    "        u_gradients, c_gradients = get_parameters_update(minibatch, hyperparameters, model_parameters)\n",
    "\n",
    "        if i % sgd_parameters.anealing_factor == 0:\n",
    "            learning_rate /= 2.0\n",
    "        \n",
    "        for index, gradient in u_gradients.items():\n",
    "            U[index] += (learning_rate * gradient).reshape(-1)\n",
    "            U[index] /= np.sqrt(np.sum(np.power(U[index], 2)))\n",
    "\n",
    "        for index, gradient in c_gradients.items():\n",
    "            V[index] += (learning_rate * gradient).reshape(-1)\n",
    "            V[index] /= np.sqrt(np.sum(np.power(V[index], 2)))\n",
    "\n",
    "        score, samples = log_prob_minibatch(minibatch, hyperparameters, model_parameters)\n",
    "        trainset_scores[i] = score / samples\n",
    "        scores_sequence.append(score)\n",
    "        \n",
    "        if liveview and i % 5 == 0:\n",
    "            update_live_view(fig, ax, scores_sequence, sgd_parameters.anealing_factor)\n",
    "                    \n",
    "        if testset is not None and i % hyperparameters.testset_measure_iterations == 0:\n",
    "            score, samples = log_prob_full(testset, hyperparameters, model_parameters)\n",
    "            testset_scores[i] = score / samples\n",
    "            \n",
    "        try:\n",
    "            open('stopFile')\n",
    "            return trainset_scores, testset_scores\n",
    "        except IOError:\n",
    "            continue\n",
    "            \n",
    "    return trainset_scores, testset_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5\n",
    "    1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_context_words(input_word, hyperparams, model_params):\n",
    "    target_index = model_params.word2index(input_word)\n",
    "    negative_indices = model_params.word2index(sample_k_words(hyperparameters.negative_words))\n",
    "    \n",
    "    top_10 = np.full(10, -np.inf)\n",
    "    top_10_words = np.full(10, None)\n",
    "    \n",
    "    for context_word in model_params._words:\n",
    "        \n",
    "        if context_word == input_word:\n",
    "            continue\n",
    "        \n",
    "        context_index = model_params.word2index(context_word)\n",
    "        log_prob = log_prob_context_with_negatives(target_index, context_index, \n",
    "                                                   negative_indices, model_params)\n",
    "\n",
    "        smallest = np.argmin(top_10)\n",
    "        if top_10[smallest] < log_prob:\n",
    "            top_10[smallest] = log_prob\n",
    "            top_10_words[smallest] = context_word\n",
    "            \n",
    "    return top_10_words[np.argsort(-top_10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_input_words(context_words, hyperparams, model_params):\n",
    "    \n",
    "    context_indices = model_params.word2index(context_words)\n",
    "    negative_indices = model_params.word2index(sample_k_words(hyperparameters.negative_words))\n",
    "    \n",
    "    top_10 = np.full(10, -np.inf)\n",
    "    top_10_words = np.full(10, None)\n",
    "    \n",
    "    for target_word in model_params._words:\n",
    "        \n",
    "        target_index = model_params.word2index(target_word)\n",
    "        log_prob = 0\n",
    "        \n",
    "        for context_index in context_indices:\n",
    "            log_prob += log_prob_context_with_negatives(target_index, context_index, \n",
    "                                                           negative_indices, model_params)\n",
    "\n",
    "        smallest = np.argmin(top_10)\n",
    "        if top_10[smallest] < log_prob:\n",
    "            top_10[smallest] = log_prob\n",
    "            top_10_words[smallest] = target_word\n",
    "            \n",
    "    return top_10_words[np.argsort(-top_10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_words(words, model_params, ax=None):\n",
    "    indices = model_params.word2index(words)\n",
    "    vectors = model_params.U[indices, :2]\n",
    "    if ax is None:\n",
    "        plt.figure()\n",
    "        plt.scatter(vectors[:, 0], vectors[:, 1])\n",
    "    else:\n",
    "        ax.scatter(vectors[:, 0], vectors[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy_solver(word1, word2, word3, model_parameters):\n",
    "    index1 = model_parameters.word2index(word1)\n",
    "    index2 = model_parameters.word2index(word2)\n",
    "    index3 = model_parameters.word2index(word3)\n",
    "    \n",
    "    comb = model_parameters.U[index1] - model_parameters.U[index2] + model_parameters.U[index3]\n",
    "    projections = model_parameters.U.dot(comb)\n",
    "    top_10 = np.argsort(-projections)[:10]\n",
    "    return model_parameters.index2word(top_10), projections[top_10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6\n",
    "\n",
    "    1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_parser = DatasetSplitParser('datasetSplit.txt')\n",
    "dataset_holder = DatasetsHolder('datasetSentences.txt', split_parser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.json', 'r') as config_file:\n",
    "    we_config = json.load(config_file)\n",
    "\n",
    "hyperparameters = Hyperparameters(**we_config['hyperparameters'])\n",
    "sgd_parameters = SGDParameters(**we_config['SGD_parameters'])\n",
    "model_parameters = ModelParameters(hyperparameters).init(dataset_holder.get_trainset())\n",
    "\n",
    "sample_k_words = get_words_sampler(dataset_holder.get_trainset(), hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_before = datetime.datetime.now()\n",
    "trainset_scores, testset_scores = LearnParamsUsingSGD(dataset_holder.get_trainset(), hyperparameters, \n",
    "                                                      sgd_parameters, model_parameters, \n",
    "                                                      dataset_holder.get_testset())\n",
    "time_after = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_likelihood, samples = log_prob_full(dataset_holder.get_trainset(), hyperparameters, model_parameters)\n",
    "final_trainset_score = log_likelihood / samples\n",
    "log_likelihood, samples = log_prob_full(dataset_holder.get_testset(), hyperparameters, model_parameters)\n",
    "final_testset_score = log_likelihood / samples\n",
    "\n",
    "output_file = open('we_run_sum.txt', 'w')\n",
    "\n",
    "output_file.write('Hyperparameters\\n-------------\\n' + str(hyperparameters) + '\\n\\n')\n",
    "output_file.write('SGD parameters\\n------------------\\n' + str(sgd_parameters) + '\\n\\n')\n",
    "output_file.write('Mean Log likelihood\\n------------------\\nTrainset: {}\\nTestset: {}\\n\\n'.format(final_trainset_score,\n",
    "                                                                                              final_testset_score))\n",
    "output_file.write('\\nTraining time\\n--------------\\n{}'.format(time_after - time_before))\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML('<span style=\"font-size: 24pt; background-color: #00ff00\">Deliverable 1</span>'))\n",
    "\n",
    "print hyperparameters\n",
    "print sgd_parameters\n",
    "\n",
    "train_x, train_y = zip(*sorted(trainset_scores.items(), key=lambda x: x[0]))\n",
    "test_x, test_y = zip(*sorted(testset_scores.items(), key=lambda x: x[0]))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_x, train_y, color='green', label='Train log-likelihood')\n",
    "plt.plot(test_x, test_y, color='red', label='Test log-likelihood')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(d):\n",
    "    \n",
    "    hyperparameters.vector_size = d\n",
    "    model_parameters = ModelParameters(hyperparameters).init(dataset_holder.get_trainset())\n",
    "    time_before = datetime.datetime.now()\n",
    "    trainset_scores, testset_scores = LearnParamsUsingSGD(dataset_holder.get_trainset(), hyperparameters, \n",
    "                                                          sgd_parameters, model_parameters)\n",
    "    time_after = datetime.datetime.now()\n",
    "    \n",
    "    log_likelihood, samples = log_prob_full(dataset_holder.get_trainset(), hyperparameters, model_parameters)\n",
    "    final_trainset_score = log_likelihood / samples\n",
    "    log_likelihood, samples = log_prob_full(dataset_holder.get_testset(), hyperparameters, model_parameters)\n",
    "    final_testset_score = log_likelihood / samples\n",
    "    \n",
    "    #final_trainset_score, final_testset_score = d ** 0.5, d ** 0.4 ################ Remove\n",
    "    \n",
    "    return time_after - time_before, final_trainset_score, final_testset_score\n",
    "\n",
    "pool = multiprocessing.Pool(5)\n",
    "\n",
    "ds = np.linspace(10, 300, 5, dtype=int)\n",
    "results = pool.map(worker, ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML('<span style=\"font-size: 24pt; background-color: #00ff00\">Deliverable 2</span>'))\n",
    "params_string = str(hyperparameters).split('\\n')\n",
    "print '\\n'.join([params_string[0]] + params_string[2:])\n",
    "print sgd_parameters\n",
    "\n",
    "f, axarr = plt.subplots(1, 2, figsize=(9.7, 3))\n",
    "\n",
    "axarr[0].plot(ds.astype(str), [results[i][0].total_seconds() for i in range(5)], marker='o')\n",
    "axarr[0].set_title('Training time (seconds)')\n",
    "axarr[0].set_xlabel('d')\n",
    "axarr[0].set_xticks(ds.astype(str))\n",
    "\n",
    "axarr[1].plot(ds.astype(str), [results[i][1] for i in range(5)], marker='o', color='green', label='Train set')\n",
    "axarr[1].plot(ds.astype(str), [results[i][2] for i in range(5)], marker='o', color='red', label='Test set')\n",
    "axarr[1].set_title('Mean Log Likelihood')\n",
    "axarr[1].set_xlabel('d')\n",
    "axarr[1].set_xticks(ds.astype(str))\n",
    "\n",
    "plt.tight_layout(w_pad=4.0)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(mb_size):\n",
    "    \n",
    "    sgd_parameters.minibatch_size = mb_size\n",
    "    model_parameters = ModelParameters(hyperparameters).init(dataset_holder.get_trainset())\n",
    "    time_before = datetime.datetime.now()\n",
    "    trainset_scores, testset_scores = LearnParamsUsingSGD(dataset_holder.get_trainset(), hyperparameters, \n",
    "                                                          sgd_parameters, model_parameters)\n",
    "    time_after = datetime.datetime.now()\n",
    "    \n",
    "    log_likelihood, samples = log_prob_full(dataset_holder.get_trainset(), hyperparameters, model_parameters)\n",
    "    final_trainset_score = log_likelihood / samples\n",
    "    log_likelihood, samples = log_prob_full(dataset_holder.get_testset(), hyperparameters, model_parameters)\n",
    "    final_testset_score = log_likelihood / samples\n",
    "    \n",
    "    #return time_after - time_before, mb_size ** 0.2, mb_size ** 0.15 ############## Remove\n",
    "\n",
    "    return time_after - time_before, final_trainset_score, final_testset_score\n",
    "\n",
    "mb_sizes = np.linspace(10, 500, 5, dtype=int)\n",
    "results = pool.map(worker, mb_sizes)\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML('<span style=\"font-size: 24pt; background-color: #00ff00\">Deliverable 3</span>'))\n",
    "print hyperparameters\n",
    "params_string = str(sgd_parameters).split('\\n')\n",
    "print '\\n'.join([params_string[0]] + params_string[2:])\n",
    "\n",
    "f, axarr = plt.subplots(1, 2, figsize=(9.7, 3))\n",
    "\n",
    "axarr[0].plot(mb_sizes.astype(str), [results[i][0].total_seconds() for i in range(5)], marker='o')\n",
    "axarr[0].set_title('Training time (seconds)')\n",
    "axarr[0].set_xlabel('MB size')\n",
    "axarr[0].set_xticks(mb_sizes.astype(str))\n",
    "\n",
    "axarr[1].plot(mb_sizes.astype(str), [results[i][1] for i in range(5)], marker='o', color='green', label='Train set')\n",
    "axarr[1].plot(mb_sizes.astype(str), [results[i][2] for i in range(5)], marker='o', color='red', label='Test set')\n",
    "axarr[1].set_title('Mean Log Likelihood')\n",
    "axarr[1].set_xlabel('MB size')\n",
    "axarr[1].set_xticks(mb_sizes.astype(str))\n",
    "\n",
    "plt.tight_layout(w_pad=4.0)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML('<span style=\"font-size: 24pt; background-color: #00ff00\">Deliverable 4</span>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['good', 'bad', 'lame', 'cool', 'exciting']\n",
    "\n",
    "for word in words:\n",
    "    print 'Target: ' + word\n",
    "    print 'Results: ' + ', '.join(predict_context_words(word, hyperparameters, model_parameters)) + '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters.vector_size = 2\n",
    "model_parameters = ModelParameters(hyperparameters).init(dataset_holder.get_trainset())\n",
    "trainset_scores, testset_scores = LearnParamsUsingSGD(dataset_holder.get_trainset(), hyperparameters, \n",
    "                                                          sgd_parameters, model_parameters)\n",
    "print hyperparameters\n",
    "print sgd_parameters\n",
    "\n",
    "############################ Use contexts embedding as well\n",
    "visualize_words(words, model_parameters)\n",
    "visualize_words(['baby', 'airplane', 'walking', 'beautiful'], model_parameters, plt.gca())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML('<span style=\"font-size: 24pt; background-color: #00ff00\">Deliverable 5</span>'))\n",
    "\n",
    "print hyperparameters\n",
    "print str(sgd_parameters) + '\\n\\n'\n",
    "\n",
    "sentences = ['the movie was suprisingly ___', '___ was really disappointing', 'knowing that she ___ was the best part']\n",
    "\n",
    "for s in sentences:\n",
    "    print 'Target: ' + s\n",
    "    predictions = predict_input_words(s.replace('___', '').split(), hyperparameters, model_parameters)\n",
    "    print 'Results: ' + ', '.join(predictions) + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML('<span style=\"font-size: 24pt; background-color: #00ff00\">Deliverable 6</span>'))\n",
    "\n",
    "print hyperparameters\n",
    "print str(sgd_parameters) + '\\n\\n'\n",
    "\n",
    "triplets = [['man', 'woman', 'men'], ['good', 'great', 'bad'], ['warm', 'cold', 'summer']]\n",
    "\n",
    "for triplet in triplets:\n",
    "    print 'Target: {} is to {} as {} is to:'.format(*triplet)\n",
    "    ############################ Use contexts embedding as well\n",
    "    results = ', '.join(analogy_solver(triplet[0], triplet[1], triplet[2], model_parameters)[0])\n",
    "    print 'Results: ' + results + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python (ml_course)",
   "language": "python",
   "name": "ml_course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "305px",
    "left": "1213px",
    "right": "20px",
    "top": "154px",
    "width": "601px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
