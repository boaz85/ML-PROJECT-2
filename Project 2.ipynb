{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Machine Learning - Project 2\n",
    "### Boaz Shvartzman, Ofir Ziv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-26T19:25:51.404529Z",
     "start_time": "2018-03-26T19:25:51.263675Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-26T19:25:52.402929Z",
     "start_time": "2018-03-26T19:25:52.382297Z"
    }
   },
   "outputs": [],
   "source": [
    "class DatasetSplitParser(object):\n",
    "    \n",
    "    def __init__(self, split_path):\n",
    "        with open(split_path, 'r')as f:\n",
    "            file_content = f.read()\n",
    "\n",
    "        self._ds_by_index = {}\n",
    "        \n",
    "        for row in file_content.split('\\n')[1:-1]:\n",
    "            index, ds = row.split(',')\n",
    "            self._ds_by_index[int(index)] = int(ds)\n",
    "            \n",
    "    def get_dataset_by_index():\n",
    "        return self._ds_by_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetsHolder(object):\n",
    "    \n",
    "    def __init__(self, sentences_path, datasplit_parser):\n",
    "        with open(sentences_path, 'r')as f:\n",
    "            file_content = f.read()\n",
    "        \n",
    "        file_content = file_content.decode(\"ascii\", \"ignore\") # Remove non-ASCII\n",
    "        file_content = re.sub(r'([^\\s\\w]|_)+', '', file_content) # Remove non-alphanumeric\n",
    "        file_content = re.sub(r'\\b\\w{1,2}\\b', '', file_content) # Remove words with less than 3 characters\n",
    "        file_content = file_content.lower() # Lowercase\n",
    "        \n",
    "        self._trainset = {}\n",
    "        self._testset = {}\n",
    "        \n",
    "        ds_by_index = datasplit_parser.get_dataset_by_index()\n",
    "        \n",
    "        for row in file_content.split('\\n')[1:-1]:\n",
    "            index, sentence = row.split('\\t')\n",
    "            \n",
    "            if ds_by_index[index] == 1:\n",
    "                self._trainset[index] = sentence.split(' ')\n",
    "            else:\n",
    "                self._testset[index] = sentence.split(' ')\n",
    "            \n",
    "    def get_trainset(self):\n",
    "        return self._trainset\n",
    "    \n",
    "    def get_testset(self):\n",
    "        return self._testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "    1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hyperparameters(object):\n",
    "    \n",
    "    def __init__(self, window_size, vector_size, negative_words, iterations, noise_distribution, random_seed):\n",
    "        self.window_size = window_size\n",
    "        self.vector_size = vector_size\n",
    "        self.negative_words = negative_words\n",
    "        self.iterations = iterations\n",
    "        self.noise_distribution = noise_distribution\n",
    "        self.random_seed = random_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2. + 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelParameters(object):\n",
    "    \n",
    "    def __init__(self, hyperparameters):\n",
    "        self._hyperparameters = hyperparameters\n",
    "        \n",
    "    def init(self, trainset):\n",
    "        self.words, self.frequencies = numpy.unique(sum(trainset.values(), []), return_counts=True)\n",
    "        vector_size = self._hyperparameters.vector_size\n",
    "        \n",
    "        def sample():\n",
    "            vectors = np.random.multivariate_normal(np.zeros(vector_size), np.identity(vector_size) * 1e-2, len(self.words))\n",
    "            return vectors / np.sqrt(np.sum(np.power(vectors, 2), axis=1)).reshape(-1, 1)\n",
    "            \n",
    "        self.U, self.V = sample(), sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class words_sampler(object):\n",
    "\n",
    "    def __init__(self, trainset, alpha):\n",
    "        self.words, self.frequencies = numpy.unique(sum(trainset.values(), []), return_counts=True)\n",
    "        factored = np.power(self.frequencies, alpha).astype(np.float64)\n",
    "        probabilities = factored / factored.sum()\n",
    "        self._cumsum = np.cumsum(probabilities)\n",
    "        \n",
    "    def __call__(self, k):\n",
    "        indices = [np.where(cumsum > np.random.rand())[0][0] for i in range(k)]\n",
    "        return self.words[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0 / (1 + np.exp(-z))\n",
    "\n",
    "def cbow(target_word, context_word, negative_words, model_params):\n",
    "    sig = np.log(sigmoid(model_params.V[context_word].T.dot(model_params.U[target_word])))\n",
    "    neg = np.sum(np.log(1 - sigmoid(model_params.V[negative_words].T.dot(model_params.U[target_word]))))\n",
    "    \n",
    "    return sig - neg"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "305px",
    "left": "1213px",
    "right": "20px",
    "top": "154px",
    "width": "601px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
