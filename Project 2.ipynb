{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Machine Learning - Project 2\n",
    "### Boaz Shvartzman, Ofir Ziv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-26T19:25:51.404529Z",
     "start_time": "2018-03-26T19:25:51.263675Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-26T19:25:52.402929Z",
     "start_time": "2018-03-26T19:25:52.382297Z"
    }
   },
   "outputs": [],
   "source": [
    "class DatasetSplitParser(object):\n",
    "    \n",
    "    def __init__(self, split_path):\n",
    "        with open(split_path, 'r')as f:\n",
    "            file_content = f.read()\n",
    "\n",
    "        self._ds_by_index = {}\n",
    "        \n",
    "        for row in file_content.split('\\n')[1:-1]:\n",
    "            index, ds = row.split(',')\n",
    "            self._ds_by_index[int(index)] = int(ds)\n",
    "            \n",
    "    def get_dataset_by_index(self):\n",
    "        return self._ds_by_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetsHolder(object):\n",
    "    \n",
    "    def __init__(self, sentences_path, datasplit_parser):\n",
    "        with open(sentences_path, 'r')as f:\n",
    "            file_content = f.read()\n",
    "        \n",
    "        file_content = file_content.decode(\"ascii\", \"ignore\") # Remove non-ASCII\n",
    "        file_content = re.sub(r'([^\\s\\w]|_)+', '', file_content) # Remove non-alphanumeric\n",
    "        file_content = file_content.lower() # Lowercase\n",
    "        \n",
    "        self._trainset = {}\n",
    "        self._testset = {}\n",
    "        \n",
    "        ds_by_index = datasplit_parser.get_dataset_by_index()\n",
    "        \n",
    "        for row in file_content.split('\\n')[1:-1]:\n",
    "            \n",
    "            index, sentence = row.split('\\t')\n",
    "            index, sentence = int(index), re.sub(r'\\b\\w{1,2}\\b', '', sentence).split()\n",
    "            \n",
    "            if ds_by_index[index] == 1:\n",
    "                self._trainset[index] = sentence\n",
    "                \n",
    "            elif ds_by_index[index] == 2:\n",
    "                self._testset[index] = sentence\n",
    "            \n",
    "    def get_trainset(self):\n",
    "        return self._trainset\n",
    "    \n",
    "    def get_testset(self):\n",
    "        return self._testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "    1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hyperparameters(object):\n",
    "    \n",
    "    def __init__(self, window_size=1, vector_size=30, negative_words=50, iterations=2000,\n",
    "                 noise_distribution='unigram', noise_dist_params={'alpha': 3/4.}, random_seed=1234):\n",
    "\n",
    "        self.window_size = window_size\n",
    "        self.vector_size = vector_size\n",
    "        self.negative_words = negative_words\n",
    "        self.iterations = iterations\n",
    "        self.noise_distribution = noise_distribution\n",
    "        self.noise_dist_params = noise_dist_params\n",
    "        self.random_seed = random_seed\n",
    "        np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2. + 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelParameters(object):\n",
    "    \n",
    "    def __init__(self, hyperparameters):\n",
    "        self._hyperparameters = hyperparameters\n",
    "        \n",
    "    def init(self, trainset):\n",
    "        self._words = np.unique(sum(trainset.values(), []))\n",
    "        vector_size = self._hyperparameters.vector_size\n",
    "        \n",
    "        def sample():\n",
    "            vectors = np.random.multivariate_normal(np.zeros(vector_size), np.identity(vector_size) * 1e-2, len(self._words))\n",
    "            return vectors / np.sqrt(np.sum(np.power(vectors, 2), axis=1)).reshape(-1, 1)\n",
    "            \n",
    "        self.U, self.V = sample(), sample()\n",
    "        \n",
    "    def word_to_index(self, words):\n",
    "        return np.where(np.isin(self._words, words))[0]\n",
    "    \n",
    "    def index_to_words(self, indices):\n",
    "        return self._words[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    4. + 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "class unigram_sampler(object):\n",
    "\n",
    "    def __init__(self, trainset, alpha):\n",
    "        self.words, self.frequencies = np.unique(sum(trainset.values(), []), return_counts=True)\n",
    "        factored = np.power(self.frequencies, alpha).astype(np.float64)\n",
    "        probabilities = factored / factored.sum()\n",
    "        self._cumsum = np.cumsum(probabilities)\n",
    "        \n",
    "    def __call__(self, k):\n",
    "        indices = [np.where(self._cumsum > np.random.rand())[0][0] for i in range(k)]\n",
    "        return self.words[indices]\n",
    "\n",
    "def get_words_sampler(dataset, hyperparams):\n",
    "    \n",
    "    if hyperparams.noise_distribution == 'unigram':\n",
    "        return unigram_sampler(dataset, hyperparams.noise_dist_params['alpha'])\n",
    "\n",
    "    else:\n",
    "        raise NotImplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ log \\left ( \\frac{1}{1 + exp(v_c^Tu_t)} \\right ) + \\sum_{j=1}^K log \\left (1 - \\frac{1}{1 + exp(v_j^Tu_t)} \\right ) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0 / (1 + np.exp(-z))\n",
    "\n",
    "def log_prob_context_with_negatives(U, V, target_word, context_word, negative_words, model_params):\n",
    "    sig = np.log(sigmoid(V[context_word].T.dot(U[target_word])))\n",
    "    neg = np.sum(np.log(1 - sigmoid(V[negative_words].T.dot(U[target_word]))))\n",
    "    \n",
    "    return sig + neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3\n",
    "\n",
    "    1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial f}{du_t} = \\left (1 - \\sigma(v_c^Tu_t \\right )v_c - \\sum_{j=1}^K \\left (1-\\sigma(v_j^Tu_t) \\right )v_j $$\n",
    "\n",
    "$$ \\frac{\\partial f}{dv_c} = \\left (1 - \\sigma(v_c^Tu_t \\right )u_t $$\n",
    "\n",
    "$$ \\frac{\\partial f}{dv_j} = - \\left (1 - \\sigma(v_j^Tu_t \\right )u_t $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prob_context_with_negatives_gradient(U, V, target_word, context_word, negative_words):\n",
    "    neg = (1 - sigmoid(V[context_word].T.dot(U[target_word])))\n",
    "    \n",
    "    dFdu = neg * V[context_word] - np.sum((1 - sigmoid(V[negative_words].dot(U[target_word]))) * V[negative_words])\n",
    "    dFdv_c = neg * U[target_word]\n",
    "    \n",
    "    neg = (1 - sigmoid(V[negative_words].T.dot(U[target_word]))).reshape(-1, 1)\n",
    "    dFdv_j = (-neg).dot(U[target_word].reshape(1, -1))\n",
    "    \n",
    "    return dFdu, dFdv_c, dFdv_j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_target_context(dataset, window_size):\n",
    "    w = np.random.choice(dataset.values(), 1)\n",
    "    target = np.random.choice(range(len(w)))\n",
    "    \n",
    "    pairs = []\n",
    "    \n",
    "    for i in range(window_size):\n",
    "        if t + i < len(w):\n",
    "            pairs.append(w[t], w[t + i])\n",
    "        if t - i >= 0:\n",
    "            pairs.append(w[t], w[t - i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_context_minibatch(dataset, minibatch_size, window_size):\n",
    "    return [\n",
    "        sample_target_context(dataset, window_size) for i in range(minibatch_size)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameters_update(minibatch_samples, model_params, U, V):\n",
    "\n",
    "    u_gradients = {}\n",
    "    c_gradients = {}\n",
    "    \n",
    "    for samples in minibatch_samples:\n",
    "        target_index = model_params.word_to_index(samples[0][0])[0]\n",
    "        for target, context in samples:\n",
    "            \n",
    "            context_index = model_params.word_to_index(context)[0]\n",
    "            negatives = sample_k_words(k)\n",
    "            negative_indeices = model_params.word_to_index(negatives)\n",
    "            \n",
    "            g_t, g_c, g_j = log_prob_context_with_negatives_gradient(target_index, context_index, \n",
    "                                                                     negative_indeices, U, V)\n",
    "            \n",
    "            u_gradients[target_index] = u_gradients.get(target_index, 0) + g_t\n",
    "            c_gradients[context_index] = c_gradients.get(context_index, 0) + g_c\n",
    "            \n",
    "            for i, index in enumerate(negative_indeices):\n",
    "                c_gradients[index] = c_gradients.get(index, 0) + g_j[i]\n",
    "                \n",
    "    return u_gradients, c_gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDParameters(object):\n",
    "    \n",
    "    def __init__(self, learning_rate, minibatch_size, anealing_factor):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.anealing_factor = anealing_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LearnParamsUsingSGD(trainset, hyperparameters, sgd_parameters, model_parameters):\n",
    "    \n",
    "    U, V = model_parameters.U, model_parameters.V\n",
    "    learning_rate = sgd_parameters.learning_rate\n",
    "    \n",
    "    for i in range(hyperparameters.iterations):\n",
    "        minibatch = get_target_context_minibatch(trainset, sgd_parameters.minibatch_size, hyperparameters.window_size)\n",
    "        u_gradients, c_gradients = get_parameters_update(minibatch, model_parameters, U, V)\n",
    "\n",
    "        if i % sgd_parameters.anealing_factor == 0:\n",
    "            learning_rate /= 2.0\n",
    "        \n",
    "        for index, gradient in u_gradients.items():\n",
    "            U[index] += learning_rate * gradient\n",
    "            \n",
    "        for index, gradient in c_gradients.items():\n",
    "            V[index] += learning_rate * gradient"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "305px",
    "left": "1213px",
    "right": "20px",
    "top": "154px",
    "width": "601px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
